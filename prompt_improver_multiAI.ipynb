{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G10hdz/promptimprover/blob/main/prompt_improver_multiAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# promptsmith_unified_final.py\n",
        "from __future__ import annotations\n",
        "from typing import Any, Dict, List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "import asyncio\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Try to import your orchestration primitives and runtime agents.\n",
        "# If not available (e.g., in Colab/VS Code sandbox), we provide drop-in shims.\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "try:\n",
        "    from agents import Agent, Runner, trace  # Provided by your stack\n",
        "except Exception:\n",
        "    # Minimal fallbacks so the file can import and RUN.\n",
        "    class Agent:  # type: ignore\n",
        "        def __init__(self, name: str, model: str, output_type: Any, instructions: str = \"\"):\n",
        "            self.name = name\n",
        "            self.model = model\n",
        "            self.output_type = output_type\n",
        "            self.instructions = instructions\n",
        "\n",
        "    class _DummyRes:\n",
        "        def __init__(self, out: Any): self.final_output = out\n",
        "\n",
        "    from contextlib import contextmanager\n",
        "    @contextmanager\n",
        "    def trace(name: str):  # type: ignore\n",
        "        yield\n",
        "\n",
        "    # Pydantic models for outputs used by agents\n",
        "    class ChatMessage(BaseModel):\n",
        "        role: str\n",
        "        content: str\n",
        "\n",
        "    class Issues(BaseModel):\n",
        "        issues: List[str] = Field(default_factory=list)\n",
        "        has_issues: bool = False\n",
        "        def model_dump(self):\n",
        "            return {\"issues\": self.issues, \"has_issues\": self.has_issues}\n",
        "\n",
        "    class FewShotIssues(Issues):\n",
        "        @classmethod\n",
        "        def no_issues(cls): return cls(issues=[], has_issues=False)\n",
        "\n",
        "    class DevRewriteOutput(BaseModel):\n",
        "        new_developer_message: str\n",
        "\n",
        "    class MessagesOutput(BaseModel):\n",
        "        messages: List[Dict[str, str]]\n",
        "\n",
        "    class EvaluationOutput(BaseModel):\n",
        "        score_improvement: int = 8\n",
        "        is_improvement: bool = True\n",
        "        explanation: str = \"OK\"\n",
        "\n",
        "    def _normalize_messages(msgs: List[ChatMessage]):  # noqa\n",
        "        return [{\"role\": m.role, \"content\": m.content} for m in msgs]\n",
        "\n",
        "    class Runner:  # type: ignore\n",
        "        @staticmethod\n",
        "        async def run(agent: Agent, payload: Any):\n",
        "            \"\"\"\n",
        "            Produce outputs consistent with the agent's declared output_type.\n",
        "            This dummy lets the workflow run end-to-end without external APIs.\n",
        "            \"\"\"\n",
        "            ot = agent.output_type\n",
        "            # Simular issues para activar el camino de rewrite y evaluación\n",
        "            if ot is Issues:\n",
        "                has = True if (\"format\" in agent.name or \"contradiction\" in agent.name) else False\n",
        "                return _DummyRes(Issues(issues=([\"dummy-issue\"] if has else []), has_issues=has))\n",
        "            if ot is FewShotIssues:\n",
        "                return _DummyRes(FewShotIssues.no_issues())\n",
        "            if ot is DevRewriteOutput:\n",
        "                text = payload if isinstance(payload, str) else json.dumps(payload)[:400]\n",
        "                msg = f\"[REWRITTEN by {agent.name}/{agent.model}] \" + (text if isinstance(text, str) else str(text))\n",
        "                return _DummyRes(DevRewriteOutput(new_developer_message=msg))\n",
        "            if ot is MessagesOutput:\n",
        "                try:\n",
        "                    data = json.loads(payload) if isinstance(payload, str) else payload\n",
        "                except Exception:\n",
        "                    data = {\"ORIGINAL_MESSAGES\": []}\n",
        "                msgs = data.get(\"ORIGINAL_MESSAGES\", [])\n",
        "                return _DummyRes(MessagesOutput(messages=msgs))\n",
        "            if ot is EvaluationOutput:\n",
        "                # Simular puntuación “razonable”\n",
        "                return _DummyRes(EvaluationOutput(score_improvement=8, is_improvement=True, explanation=\"Simulated OK\"))\n",
        "            # Default: return the payload text\n",
        "            return _DummyRes(f\"[{agent.name}:{agent.model}] {str(payload)[:200]}\")\n",
        "\n",
        "    # Define default agents (names and models kept to your spec)\n",
        "    dev_contradiction_checker = Agent(\n",
        "        name=\"contradiction_detector\", model=\"gpt-5-mini\", output_type=Issues,\n",
        "        instructions=\"Detect contradictions and return Issues JSON.\"\n",
        "    )\n",
        "    format_checker = Agent(\n",
        "        name=\"format_checker\", model=\"gpt-5-mini\", output_type=Issues,\n",
        "        instructions=\"Detect format/spec problems and return Issues JSON.\"\n",
        "    )\n",
        "    fewshot_consistency_checker = Agent(\n",
        "        name=\"fewshot_consistency_checker\", model=\"gpt-5-mini\", output_type=FewShotIssues,\n",
        "        instructions=\"Check few-shot consistency and return FewShotIssues JSON.\"\n",
        "    )\n",
        "    dev_rewriter = Agent(\n",
        "        name=\"dev_rewriter\", model=\"gpt-5\", output_type=DevRewriteOutput,\n",
        "        instructions=\"Rewrite developer message to fix issues; return new_developer_message.\"\n",
        "    )\n",
        "    fewshot_rewriter = Agent(\n",
        "        name=\"fewshot_rewriter\", model=\"gpt-5\", output_type=MessagesOutput,\n",
        "        instructions=\"Rewrite few-shot messages; return messages list.\"\n",
        "    )\n",
        "    prompt_evaluator_agent = Agent(\n",
        "        name=\"prompt_evaluator\", model=\"gpt-5\", output_type=EvaluationOutput,\n",
        "        instructions=\"Evaluate improvement; output score_improvement, is_improvement, explanation.\"\n",
        "    )\n",
        "\n",
        "# If your runtime exists, import it (overrides shims above)\n",
        "try:\n",
        "    from main_shared_runtime import (  # <- change path if needed\n",
        "        dev_contradiction_checker,\n",
        "        format_checker,\n",
        "        fewshot_consistency_checker,\n",
        "        dev_rewriter,\n",
        "        fewshot_rewriter,\n",
        "        prompt_evaluator_agent,\n",
        "        ChatMessage, Issues, FewShotIssues,\n",
        "        _normalize_messages,\n",
        "    )\n",
        "except Exception:\n",
        "    # keep using the shims defined above\n",
        "    pass\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Multi-platform output schema (OpenAI · Gemini · Claude)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "class MultiOptimizedOutput(BaseModel):\n",
        "    \"\"\"Platform-specific optimized prompts\"\"\"\n",
        "    openai_optimized_prompt: str = Field(description=\"Prompt optimized for OpenAI (GPT-5, GPT-4o, o1)\")\n",
        "    gemini_optimized_prompt: str = Field(description=\"Prompt optimized for Google Gemini\")\n",
        "    claude_optimized_prompt: str = Field(description=\"Prompt optimized for Anthropic Claude\")\n",
        "    openai_best_practices: List[str] = Field(description=\"OpenAI-specific practices applied\")\n",
        "    gemini_best_practices: List[str] = Field(description=\"Gemini-specific practices applied\")\n",
        "    claude_best_practices: List[str] = Field(description=\"Claude-specific practices applied\")\n",
        "\n",
        "# Platform optimizers (these RUN with GPT-5 to WRITE optimized prompts)\n",
        "openai_optimizer = Agent(\n",
        "    name=\"openai_optimizer\", model=\"gpt-5\", output_type=str,\n",
        "    instructions=r\"\"\"\n",
        "You are an OpenAI Prompt Optimization Specialist.\n",
        "Task: Transform UNIFIED_PROMPT into an OpenAI-optimized prompt (GPT-5, GPT-4o, o1).\n",
        "Apply these practices:\n",
        "- System/User separation\n",
        "- Numbered steps for complex tasks\n",
        "- Output format specified at the top\n",
        "- Clear delimiters (### or ```), no markdown fences around JSON when in JSON mode\n",
        "- 1–2 few-shot examples only if format is complex\n",
        "- Temperature guidance (0 for factual, ~0.7 creative)\n",
        "- Token/length constraints when needed\n",
        "- Start with a specific role (\"You are a …\")\n",
        "- State constraints BEFORE instructions\n",
        "- If structured output, explicitly request JSON and give a minimal schema\n",
        "Input: UNIFIED_PROMPT\n",
        "Output: ONLY the OpenAI-optimized prompt as plain text.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "gemini_optimizer = Agent(\n",
        "    name=\"gemini_optimizer\", model=\"gpt-5\", output_type=str,\n",
        "    instructions=r\"\"\"\n",
        "You are a Google Gemini Prompt Optimization Specialist.\n",
        "Transform UNIFIED_PROMPT into a Gemini-optimized prompt.\n",
        "Apply these practices:\n",
        "- Conversational, context-rich framing\n",
        "- Multi-turn awareness (assume prior chat)\n",
        "- Encourage explicit reasoning (\"think through\")\n",
        "- Markdown for structure and scannability\n",
        "- Multimodal hints when applicable (images/docs)\n",
        "- Positive framing (avoid excessive \"don't\")\n",
        "- Safety/educational intent explicit\n",
        "- Dialogue-style examples (user/assistant) if using few-shot\n",
        "Input: UNIFIED_PROMPT\n",
        "Output: ONLY the Gemini-optimized prompt as plain text.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "claude_optimizer = Agent(\n",
        "    name=\"claude_optimizer\", model=\"gpt-5\", output_type=str,\n",
        "    instructions=r\"\"\"\n",
        "You are an Anthropic Claude Prompt Optimization Specialist.\n",
        "Transform UNIFIED_PROMPT into a Claude-optimized prompt.\n",
        "Claude best practices to apply:\n",
        "- Keep core instructions in a single, strong system-style preamble.\n",
        "- Prefer explicit sections with XML-like tags or clear headers:\n",
        "  <role>…</role>, <task>…</task>, <context>…</context>, <constraints>…</constraints>, <output>…</output>\n",
        "- Set safety/ethics intent and audience explicitly (HHH: helpful, honest, harmless).\n",
        "- Use precise guardrails (\"Do not reveal chain-of-thought; return only final answer or brief rationale\").\n",
        "- If structured output: ask for strict JSON with a minimal schema and an example; forbid extra prose.\n",
        "- Use few-shot exemplars only when critical; show input/output pairs; keep them compact.\n",
        "- Provide length/format limits (words, bullets, tables) early.\n",
        "- Prefer positive phrasing; list what to include before what to avoid.\n",
        "- If stepwise reasoning is required, put it in a hidden scratchpad section and instruct not to expose it.\n",
        "- Be deterministic: recommend low temperature for factual tasks.\n",
        "Input: UNIFIED_PROMPT\n",
        "Output: ONLY the Claude-optimized prompt as plain text.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "async def create_multi_optimized_outputs(unified_prompt: str) -> MultiOptimizedOutput:\n",
        "    \"\"\"Generate platform-specific optimized prompts for OpenAI, Gemini, and Claude.\"\"\"\n",
        "    print(\"  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\")\n",
        "    openai_task = Runner.run(openai_optimizer, unified_prompt)\n",
        "    gemini_task = Runner.run(gemini_optimizer, unified_prompt)\n",
        "    claude_task = Runner.run(claude_optimizer, unified_prompt)\n",
        "    results = await asyncio.gather(openai_task, gemini_task, claude_task)\n",
        "    openai_optimized = getattr(results[0], \"final_output\", \"\")\n",
        "    gemini_optimized = getattr(results[1], \"final_output\", \"\")\n",
        "    claude_optimized = getattr(results[2], \"final_output\", \"\")\n",
        "    return MultiOptimizedOutput(\n",
        "        openai_optimized_prompt=openai_optimized,\n",
        "        gemini_optimized_prompt=gemini_optimized,\n",
        "        claude_optimized_prompt=claude_optimized,\n",
        "        openai_best_practices=[\n",
        "            \"System/User separation\",\n",
        "            \"Numbered steps\",\n",
        "            \"Output format first\",\n",
        "            \"Delimiters for sections\",\n",
        "            \"Temperature guidance\",\n",
        "            \"JSON mode with schema when needed\",\n",
        "        ],\n",
        "        gemini_best_practices=[\n",
        "            \"Conversational, context-rich framing\",\n",
        "            \"Multi-turn awareness\",\n",
        "            \"Markdown structure\",\n",
        "            \"Positive framing\",\n",
        "            \"Explicit reasoning prompts\",\n",
        "        ],\n",
        "        claude_best_practices=[\n",
        "            \"Single strong preamble (system-style)\",\n",
        "            \"XML-like tagged sections\",\n",
        "            \"HHH intent & audience set\",\n",
        "            \"Hide chain-of-thought; final answer only\",\n",
        "            \"Strict JSON with schema + example\",\n",
        "            \"Concise few-shot IO pairs only when needed\",\n",
        "            \"Early length/format limits\",\n",
        "            \"Positive phrasing + explicit inclusions\",\n",
        "            \"Optional hidden scratchpad with non-exposure rule\",\n",
        "            \"Low temperature for factual tasks\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Unified optimize wrapper (adds multi-platform outputs after rewrite)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "async def optimize_prompt_parallel_with_platforms(\n",
        "    developer_message: str,\n",
        "    messages: List[\"ChatMessage\"],\n",
        "    *, create_platform_outputs: bool = True,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Wraps your existing optimize steps and appends platform-specific outputs\n",
        "    when a rewrite occurs. Works with GPT-5-mini (checkers) + GPT-5 (writer/judge).\n",
        "    \"\"\"\n",
        "    with trace(\"optimize_prompt_workflow\"):\n",
        "        # 1) Run checkers in parallel\n",
        "        tasks = [\n",
        "            Runner.run(dev_contradiction_checker, developer_message),\n",
        "            Runner.run(format_checker, developer_message),\n",
        "        ]\n",
        "        if messages:\n",
        "            fs_input = {\n",
        "                \"DEVELOPER_MESSAGE\": developer_message,\n",
        "                \"USER_EXAMPLES\": [m.content for m in messages if m.role == \"user\"],\n",
        "                \"ASSISTANT_EXAMPLES\": [m.content for m in messages if m.role == \"assistant\"],\n",
        "            }\n",
        "            tasks.append(Runner.run(fewshot_consistency_checker, json.dumps(fs_input)))\n",
        "\n",
        "        results = await asyncio.gather(*tasks)\n",
        "        cd_issues: Issues = results[0].final_output\n",
        "        fi_issues: Issues = results[1].final_output\n",
        "        fs_issues: FewShotIssues = (results[2].final_output if len(results) > 2 else FewShotIssues.no_issues())\n",
        "\n",
        "        # 2) Rewrite if needed\n",
        "        final_prompt = developer_message\n",
        "        prompt_was_rewritten = False\n",
        "        if getattr(cd_issues, \"has_issues\", False) or getattr(fi_issues, \"has_issues\", False):\n",
        "            pr_input = {\n",
        "                \"ORIGINAL_DEVELOPER_MESSAGE\": developer_message,\n",
        "                \"CONTRADICTION_ISSUES\": cd_issues.model_dump(),\n",
        "                \"FORMAT_ISSUES\": fi_issues.model_dump(),\n",
        "            }\n",
        "            pr_res = await Runner.run(dev_rewriter, json.dumps(pr_input))\n",
        "            new_msg = getattr(pr_res.final_output, \"new_developer_message\", None)\n",
        "            if new_msg is None and isinstance(pr_res.final_output, dict):\n",
        "                new_msg = pr_res.final_output.get(\"new_developer_message\", developer_message)\n",
        "            final_prompt = new_msg or developer_message\n",
        "            if final_prompt != developer_message:\n",
        "                prompt_was_rewritten = True\n",
        "\n",
        "        # Few-shot rewrite if needed\n",
        "        final_messages: List[ChatMessage] | List[Dict[str, str]] = messages\n",
        "        few_shots_were_rewritten = False\n",
        "        if getattr(fs_issues, \"has_issues\", False):\n",
        "            mr_input = {\n",
        "                \"NEW_DEVELOPER_MESSAGE\": final_prompt,\n",
        "                \"ORIGINAL_MESSAGES\": _normalize_messages(messages),\n",
        "                \"FEW_SHOT_ISSUES\": fs_issues.model_dump(),\n",
        "            }\n",
        "            mr_res = await Runner.run(fewshot_rewriter, json.dumps(mr_input))\n",
        "            msgs = getattr(mr_res.final_output, \"messages\", None)\n",
        "            if msgs is None and isinstance(mr_res.final_output, dict):\n",
        "                msgs = mr_res.final_output.get(\"messages\", _normalize_messages(messages))\n",
        "            final_messages = msgs\n",
        "            few_shots_were_rewritten = True\n",
        "\n",
        "        # 3) Platform-specific outputs\n",
        "        multi_output = None\n",
        "        if prompt_was_rewritten and create_platform_outputs:\n",
        "            try:\n",
        "                multi_output = await create_multi_optimized_outputs(final_prompt)\n",
        "                print(\"  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠ Error creating platform outputs: {e}\")\n",
        "                multi_output = None\n",
        "\n",
        "        # 4) Evaluate improvement (for unified only; per-platform eval is below in run loop)\n",
        "        evaluation_results = None\n",
        "        if prompt_was_rewritten:\n",
        "            try:\n",
        "                eval_input = {\"ORIGINAL_PROMPT\": developer_message, \"NEW_PROMPT\": final_prompt}\n",
        "                eval_res = await Runner.run(prompt_evaluator_agent, json.dumps(eval_input))\n",
        "                payload = getattr(eval_res, \"final_output\", {})\n",
        "                if hasattr(payload, \"model_dump\"):\n",
        "                    evaluation_results = payload.model_dump()\n",
        "                else:\n",
        "                    evaluation_results = payload\n",
        "            except Exception as e:\n",
        "                evaluation_results = {\"error\": str(e)}\n",
        "\n",
        "        # 5) Final payload\n",
        "        changes_made = prompt_was_rewritten or few_shots_were_rewritten\n",
        "        return {\n",
        "            \"changes_made\": changes_made,\n",
        "            \"unified_prompt\": final_prompt,\n",
        "            \"openai_optimized\": (multi_output.openai_optimized_prompt if multi_output else final_prompt),\n",
        "            \"gemini_optimized\": (multi_output.gemini_optimized_prompt if multi_output else final_prompt),\n",
        "            \"claude_optimized\": (multi_output.claude_optimized_prompt if multi_output else final_prompt),\n",
        "            \"openai_best_practices\": (multi_output.openai_best_practices if multi_output else []),\n",
        "            \"gemini_best_practices\": (multi_output.gemini_best_practices if multi_output else []),\n",
        "            \"claude_best_practices\": (multi_output.claude_best_practices if multi_output else []),\n",
        "            \"new_messages\": _normalize_messages(final_messages) if isinstance(final_messages, list) else final_messages,\n",
        "            \"evaluation\": evaluation_results,\n",
        "        }\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Pretty table helpers\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "async def _evaluate_platforms(original_prompt: str, out: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Evalúa con prompt_evaluator_agent cada una de las versiones:\n",
        "    unified, openai, gemini, claude. Devuelve dict con score/is_improvement.\n",
        "    \"\"\"\n",
        "    versions = {\n",
        "        \"unified\": out.get(\"unified_prompt\", \"\"),\n",
        "        \"openai\": out.get(\"openai_optimized\", \"\"),\n",
        "        \"gemini\": out.get(\"gemini_optimized\", \"\"),\n",
        "        \"claude\": out.get(\"claude_optimized\", \"\"),\n",
        "    }\n",
        "\n",
        "    results: Dict[str, Dict[str, Any]] = {}\n",
        "    for name, candidate in versions.items():\n",
        "        try:\n",
        "            eval_input = {\"ORIGINAL_PROMPT\": original_prompt, \"NEW_PROMPT\": candidate}\n",
        "            eval_res = await Runner.run(prompt_evaluator_agent, json.dumps(eval_input))\n",
        "            payload = getattr(eval_res, \"final_output\", {})\n",
        "            if hasattr(payload, \"model_dump\"):\n",
        "                payload = payload.model_dump()\n",
        "            results[name] = {\n",
        "                \"score\": payload.get(\"score_improvement\", 0),\n",
        "                \"is_improvement\": payload.get(\"is_improvement\", False),\n",
        "                \"explanation\": payload.get(\"explanation\", \"N/A\"),\n",
        "            }\n",
        "        except Exception as e:\n",
        "            results[name] = {\"score\": 0, \"is_improvement\": False, \"explanation\": f\"error: {e}\"}\n",
        "    return results\n",
        "\n",
        "def _print_attempt_table(attempt_idx: int, eval_map: Dict[str, Dict[str, Any]]) -> None:\n",
        "    print(f\"\\n   Attempt {attempt_idx}: scores by platform\")\n",
        "    print(\"   \" + \"-\" * 58)\n",
        "    print(f\"   {'Platform':<12} {'Improved':<10} {'Score':<8} Explanation\")\n",
        "    print(\"   \" + \"-\" * 58)\n",
        "    for name in [\"unified\", \"openai\", \"gemini\", \"claude\"]:\n",
        "        data = eval_map.get(name, {\"score\": 0, \"is_improvement\": False, \"explanation\": \"N/A\"})\n",
        "        improved = \"✓\" if data.get(\"is_improvement\") else \"✗\"\n",
        "        score = data.get(\"score\", 0)\n",
        "        expl = str(data.get(\"explanation\", \"\"))[:60]\n",
        "        print(f\"   {name:<12} {improved:<10} {score:<8} {expl}\")\n",
        "    print(\"   \" + \"-\" * 58)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Test suite — 6 impressive, realistic cases\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "TEST_CASES = [\n",
        "    {\n",
        "        \"id\": \"medical_guideline_explainer\",\n",
        "        \"prompt\": \"Explain hypertension management for doctors vs patients using clear structure and empathy.\",\n",
        "        \"messages\": [],\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"financial_summary_generator\",\n",
        "        \"prompt\": \"Summarize quarterly earnings for a fintech startup highlighting trends and growth metrics for investors.\",\n",
        "        \"messages\": [],\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"educational_curriculum_builder\",\n",
        "        \"prompt\": \"Design a 4-week AI ethics curriculum for high school students blending philosophy and technology.\",\n",
        "        \"messages\": [],\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"creative_story_corporate_metaphor\",\n",
        "        \"prompt\": \"Write a short allegorical story about burnout recovery using an office as metaphor for a phoenix nest.\",\n",
        "        \"messages\": [],\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"software_doc_refactor\",\n",
        "        \"prompt\": \"Refactor documentation for an open-source project to improve onboarding clarity and code examples.\",\n",
        "        \"messages\": [],\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"data_analysis_causal_reasoning\",\n",
        "        \"prompt\": \"Analyze customer churn dataset explaining causal insights rather than correlations, with clear reasoning.\",\n",
        "        \"messages\": [],\n",
        "    },\n",
        "]\n",
        "\n",
        "# Número de intentos por test (para ver evolución)\n",
        "NUM_TRIALS = 3\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Runner principal con múltiples intentos por test\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "async def run_all_tests() -> List[Dict[str, Any]]:\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"🚀 Running PromptSmith Unified (GPT-5-mini core + Multi-Platform Outputs)\")\n",
        "    print(\"=\"*90, flush=True)\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    total = len(TEST_CASES)\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    for i, test in enumerate(TEST_CASES, start=1):\n",
        "        print(f\"\\n[{i}/{total}] ▶ {test['id']}\")\n",
        "        print(\"-\" * 90)\n",
        "        print(f\"🧩 Original prompt:\\n{test['prompt']}\\n\", flush=True)\n",
        "\n",
        "        attempts_history = []\n",
        "        best_score_overall = -1.0\n",
        "        best_attempt_idx = 0\n",
        "\n",
        "        for attempt in range(1, NUM_TRIALS + 1):\n",
        "            try:\n",
        "                out = await optimize_prompt_parallel_with_platforms(\n",
        "                    developer_message=test[\"prompt\"],\n",
        "                    messages=[ChatMessage(role=\"user\", content=test[\"prompt\"])] if 'ChatMessage' in globals() else [],\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error in test {test['id']} attempt {attempt}: {e}\", flush=True)\n",
        "                continue\n",
        "\n",
        "            # Evalúa por plataforma\n",
        "            eval_map = await _evaluate_platforms(test[\"prompt\"], out)\n",
        "\n",
        "            # Imprime preview + diferencias por plataforma\n",
        "            print(f\"\\n🧠 Unified prompt (attempt {attempt} preview):\")\n",
        "            print((out.get(\"unified_prompt\", \"\") or \"\")[:400] + \"...\\n\")\n",
        "\n",
        "            same_u = lambda k: \"Same as unified\" if out.get(k, \"\") == out.get(\"unified_prompt\", \"\") else \"OK\"\n",
        "            print(f\"🤖 Platform-specific versions (attempt {attempt}):\")\n",
        "            print(\"  • OpenAI  →\", same_u(\"openai_optimized\"))\n",
        "            print(\"  • Gemini  →\", same_u(\"gemini_optimized\"))\n",
        "            print(\"  • Claude  →\", same_u(\"claude_optimized\"))\n",
        "\n",
        "            # Muestra tabla por intento\n",
        "            _print_attempt_table(attempt, eval_map)\n",
        "\n",
        "            # Track del mejor intento (promedio de plataformas)\n",
        "            avg_score_this_attempt = (\n",
        "                eval_map[\"unified\"][\"score\"]\n",
        "                + eval_map[\"openai\"][\"score\"]\n",
        "                + eval_map[\"gemini\"][\"score\"]\n",
        "                + eval_map[\"claude\"][\"score\"]\n",
        "            ) / 4.0\n",
        "            if avg_score_this_attempt > best_score_overall:\n",
        "                best_score_overall = avg_score_this_attempt\n",
        "                best_attempt_idx = attempt\n",
        "\n",
        "            attempts_history.append({\n",
        "                \"attempt\": attempt,\n",
        "                \"unified\": eval_map[\"unified\"],\n",
        "                \"openai\": eval_map[\"openai\"],\n",
        "                \"gemini\": eval_map[\"gemini\"],\n",
        "                \"claude\": eval_map[\"claude\"],\n",
        "                \"unified_preview\": (out.get(\"unified_prompt\", \"\") or \"\")[:200],\n",
        "            })\n",
        "\n",
        "        # Consolidado de test (tomamos el último intento como “estado final” visible)\n",
        "        if attempts_history:\n",
        "            last = attempts_history[-1]\n",
        "            improvement_score = last[\"unified\"][\"score\"]\n",
        "            is_improvement = last[\"unified\"][\"is_improvement\"]\n",
        "        else:\n",
        "            improvement_score = 0\n",
        "            is_improvement = False\n",
        "\n",
        "        results.append({\n",
        "            \"test_id\": test[\"id\"],\n",
        "            \"attempts\": attempts_history,\n",
        "            \"best_attempt\": best_attempt_idx,\n",
        "            \"best_avg_score\": best_score_overall,\n",
        "            \"final_unified_score\": improvement_score,\n",
        "            \"final_unified_is_improvement\": is_improvement,\n",
        "        })\n",
        "\n",
        "        print(f\"\\n🏅 Best attempt for '{test['id']}': Attempt {best_attempt_idx} (avg score={best_score_overall:.1f})\")\n",
        "        print(\"=\" * 90, flush=True)\n",
        "\n",
        "    print(\"\\n🏁 All tests completed.\")\n",
        "    end_time = datetime.datetime.now()\n",
        "    print(f\"🕒 Duration: {end_time - start_time}\")\n",
        "    print(f\"✅ Total processed: {len(results)}\", flush=True)\n",
        "\n",
        "    # Resumen global (usa mejor intento por test)\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"RESULTADOS FINALES — Mejor intento por test (avg score across platforms)\")\n",
        "    print(\"=\"*90)\n",
        "    print(f\"{'Test Case':<35} {'BestAttempt':<12} {'BestAvgScore':<14}\")\n",
        "    print(\"-\" * 90)\n",
        "    for r in results:\n",
        "        print(f\"{r['test_id']:<35} {r['best_attempt']:<12} {r['best_avg_score']:<14.1f}\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    # Guardar resultados\n",
        "    out_name = f\"promptsmith_unified_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    try:\n",
        "        with open(out_name, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"\\n💾 Results saved to: {out_name}\", flush=True)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not save results: {e}\", flush=True)\n",
        "\n",
        "    return results\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Colab/VSCode entrypoint\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        asyncio.run(run_all_tests())\n",
        "    except RuntimeError:\n",
        "        # Colab event loop already running\n",
        "        import nest_asyncio, asyncio as _asyncio\n",
        "        nest_asyncio.apply()\n",
        "        _asyncio.get_event_loop().run_until_complete(run_all_tests())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UugvumOchpuK",
        "outputId": "4da081d2-ee88-44cd-c161-c3730fadd582"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================================================================================\n",
            "🚀 Running PromptSmith Unified (GPT-5-mini core + Multi-Platform Outputs)\n",
            "==========================================================================================\n",
            "\n",
            "[1/6] ▶ medical_guideline_explainer\n",
            "------------------------------------------------------------------------------------------\n",
            "🧩 Original prompt:\n",
            "Explain hypertension management for doctors vs patients using clear structure and empathy.\n",
            "\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 1 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Explain hypertension management for doctors vs patients using clear structure and empathy.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 1):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 1: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 2 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Explain hypertension management for doctors vs patients using clear structure and empathy.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 2):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 2: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 3 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Explain hypertension management for doctors vs patients using clear structure and empathy.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 3):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 3: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "\n",
            "🏅 Best attempt for 'medical_guideline_explainer': Attempt 1 (avg score=8.0)\n",
            "==========================================================================================\n",
            "\n",
            "[2/6] ▶ financial_summary_generator\n",
            "------------------------------------------------------------------------------------------\n",
            "🧩 Original prompt:\n",
            "Summarize quarterly earnings for a fintech startup highlighting trends and growth metrics for investors.\n",
            "\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 1 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Summarize quarterly earnings for a fintech startup highlighting trends and growth metrics for investors.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 1):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 1: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 2 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Summarize quarterly earnings for a fintech startup highlighting trends and growth metrics for investors.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 2):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 2: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 3 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Summarize quarterly earnings for a fintech startup highlighting trends and growth metrics for investors.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 3):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 3: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "\n",
            "🏅 Best attempt for 'financial_summary_generator': Attempt 1 (avg score=8.0)\n",
            "==========================================================================================\n",
            "\n",
            "[3/6] ▶ educational_curriculum_builder\n",
            "------------------------------------------------------------------------------------------\n",
            "🧩 Original prompt:\n",
            "Design a 4-week AI ethics curriculum for high school students blending philosophy and technology.\n",
            "\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 1 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Design a 4-week AI ethics curriculum for high school students blending philosophy and technology.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 1):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 1: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 2 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Design a 4-week AI ethics curriculum for high school students blending philosophy and technology.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 2):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 2: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 3 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Design a 4-week AI ethics curriculum for high school students blending philosophy and technology.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 3):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 3: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "\n",
            "🏅 Best attempt for 'educational_curriculum_builder': Attempt 1 (avg score=8.0)\n",
            "==========================================================================================\n",
            "\n",
            "[4/6] ▶ creative_story_corporate_metaphor\n",
            "------------------------------------------------------------------------------------------\n",
            "🧩 Original prompt:\n",
            "Write a short allegorical story about burnout recovery using an office as metaphor for a phoenix nest.\n",
            "\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 1 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Write a short allegorical story about burnout recovery using an office as metaphor for a phoenix nest.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 1):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 1: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 2 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Write a short allegorical story about burnout recovery using an office as metaphor for a phoenix nest.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 2):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 2: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 3 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Write a short allegorical story about burnout recovery using an office as metaphor for a phoenix nest.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 3):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 3: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "\n",
            "🏅 Best attempt for 'creative_story_corporate_metaphor': Attempt 1 (avg score=8.0)\n",
            "==========================================================================================\n",
            "\n",
            "[5/6] ▶ software_doc_refactor\n",
            "------------------------------------------------------------------------------------------\n",
            "🧩 Original prompt:\n",
            "Refactor documentation for an open-source project to improve onboarding clarity and code examples.\n",
            "\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 1 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Refactor documentation for an open-source project to improve onboarding clarity and code examples.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 1):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 1: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 2 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Refactor documentation for an open-source project to improve onboarding clarity and code examples.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 2):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 2: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 3 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Refactor documentation for an open-source project to improve onboarding clarity and code examples.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 3):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 3: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "\n",
            "🏅 Best attempt for 'software_doc_refactor': Attempt 1 (avg score=8.0)\n",
            "==========================================================================================\n",
            "\n",
            "[6/6] ▶ data_analysis_causal_reasoning\n",
            "------------------------------------------------------------------------------------------\n",
            "🧩 Original prompt:\n",
            "Analyze customer churn dataset explaining causal insights rather than correlations, with clear reasoning.\n",
            "\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 1 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Analyze customer churn dataset explaining causal insights rather than correlations, with clear reasoning.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 1):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 1: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 2 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Analyze customer churn dataset explaining causal insights rather than correlations, with clear reasoning.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 2):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 2: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "  → Generating platform-specific optimizations (OpenAI/Gemini/Claude)…\n",
            "  ✓ Platform-specific versions (OpenAI/Gemini/Claude) generated\n",
            "\n",
            "🧠 Unified prompt (attempt 3 preview):\n",
            "[REWRITTEN by dev_rewriter/gpt-5] {\"ORIGINAL_DEVELOPER_MESSAGE\": \"Analyze customer churn dataset explaining causal insights rather than correlations, with clear reasoning.\", \"CONTRADICTION_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}, \"FORMAT_ISSUES\": {\"issues\": [\"dummy-issue\"], \"has_issues\": true}}...\n",
            "\n",
            "🤖 Platform-specific versions (attempt 3):\n",
            "  • OpenAI  → OK\n",
            "  • Gemini  → OK\n",
            "  • Claude  → OK\n",
            "\n",
            "   Attempt 3: scores by platform\n",
            "   ----------------------------------------------------------\n",
            "   Platform     Improved   Score    Explanation\n",
            "   ----------------------------------------------------------\n",
            "   unified      ✓          8        Simulated OK\n",
            "   openai       ✓          8        Simulated OK\n",
            "   gemini       ✓          8        Simulated OK\n",
            "   claude       ✓          8        Simulated OK\n",
            "   ----------------------------------------------------------\n",
            "\n",
            "🏅 Best attempt for 'data_analysis_causal_reasoning': Attempt 1 (avg score=8.0)\n",
            "==========================================================================================\n",
            "\n",
            "🏁 All tests completed.\n",
            "🕒 Duration: 0:00:00.037145\n",
            "✅ Total processed: 6\n",
            "\n",
            "==========================================================================================\n",
            "RESULTADOS FINALES — Mejor intento por test (avg score across platforms)\n",
            "==========================================================================================\n",
            "Test Case                           BestAttempt  BestAvgScore  \n",
            "------------------------------------------------------------------------------------------\n",
            "medical_guideline_explainer         1            8.0           \n",
            "financial_summary_generator         1            8.0           \n",
            "educational_curriculum_builder      1            8.0           \n",
            "creative_story_corporate_metaphor   1            8.0           \n",
            "software_doc_refactor               1            8.0           \n",
            "data_analysis_causal_reasoning      1            8.0           \n",
            "==========================================================================================\n",
            "\n",
            "💾 Results saved to: promptsmith_unified_results_20251024_000417.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3807444364.py:563: RuntimeWarning: coroutine 'run_all_tests' was never awaited\n",
            "  _asyncio.get_event_loop().run_until_complete(run_all_tests())\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    }
  ]
}